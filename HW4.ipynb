{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chenkaiwen111811/PL-Repo/blob/main/HW4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install gspread gspread_dataframe google-auth google-auth-oauthlib google-auth-httplib2 \\\n",
        "               gradio pandas beautifulsoup4 google-generativeai python-dateutil \\\n",
        "               jieba scikit-learn"
      ],
      "metadata": {
        "id": "4essvnf0rXm_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time, uuid, re, json, datetime\n",
        "from datetime import datetime as dt, timedelta\n",
        "from dateutil.tz import gettz\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import Counter, defaultdict # <-- ç¢ºä¿ Counter å’Œ defaultdict éƒ½å·²åŒ¯å…¥\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Google Auth & Sheets\n",
        "from google.colab import auth\n",
        "import gspread\n",
        "from gspread_dataframe import set_with_dataframe, get_as_dataframe\n",
        "from google.auth.transport.requests import Request\n",
        "from google.oauth2 import service_account\n",
        "from google.auth import default"
      ],
      "metadata": {
        "id": "0ChJ5UQirXpk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "# å¾ Colab Secrets ä¸­ç²å– API é‡‘é‘°\n",
        "api_key = userdata.get('gemini')\n",
        "model = None # åˆå§‹åŒ–\n",
        "\n",
        "if api_key:\n",
        "    genai.configure(api_key=api_key)\n",
        "    # ç«‹å³åˆå§‹åŒ– modelï¼Œä¾›å¾ŒçºŒ AI Plan ä½¿ç”¨\n",
        "    try:\n",
        "        model = genai.GenerativeModel('gemini-2.5-pro')\n",
        "        print(\"Gemini 'gemini-2.5-pro' åˆå§‹åŒ–æˆåŠŸã€‚\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Gemini 'gemini-2.5-pro' åˆå§‹åŒ–å¤±æ•—: {e}ã€‚å˜—è©¦ 'gemini-1.5-flash'...\")\n",
        "        try:\n",
        "            model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "            print(\"Gemini 'gemini-1.5-flash' åˆå§‹åŒ–æˆåŠŸã€‚\")\n",
        "        except Exception as e2:\n",
        "            print(f\"âš ï¸ Gemini 'gemini-1.5-flash' åˆå§‹åŒ–å¤±æ•—: {e2}ã€‚AI Plan åŠŸèƒ½å°‡ç„¡æ³•ä½¿ç”¨ã€‚\")\n",
        "else:\n",
        "    print(\"âš ï¸ æœªåœ¨ Colab Secrets ä¸­æ‰¾åˆ° 'gemini' API é‡‘é‘°ã€‚AI Plan åŠŸèƒ½å°‡ç„¡æ³•ä½¿ç”¨ã€‚\")"
      ],
      "metadata": {
        "id": "krldyDY9rXsb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daccbfe8-d46b-43b9-8511-abbbd759d3ea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini 'gemini-2.5-pro' åˆå§‹åŒ–æˆåŠŸã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SHEET_URL = \"https://docs.google.com/spreadsheets/d/1jR3qRQr2ZvWYKNuv8wen_-eTZWdc5a-LLvH7iymn2zw/edit?usp=sharing\"\n",
        "WORKSHEET_NAME = \"å·¥ä½œè¡¨4\" # é€™æ˜¯æ‚¨çš„è©¦ç®—è¡¨æª”æ¡ˆåç¨±\n",
        "TIMEZONE = \"Asia/Taipei\"\n",
        "\n",
        "PTT_HEADER = [\n",
        "    \"post_id\",\"title\",\"url\",\"date\",\"author\",\"nrec\",\"created_at\",\n",
        "    \"fetched_at\",\"content\"\n",
        "]\n",
        "# ã€å·²æ¢å¾©ã€‘ä½¿ç”¨ TF-IDF ç‰ˆæœ¬çš„å®Œæ•´ Header\n",
        "TERMS_HEADER = [\"term\",\"freq\",\"df_count\",\"tfidf_mean\",\"examples\"]\n",
        "\n",
        "def ensure_spreadsheet(name):\n",
        "    try:\n",
        "        sh = gc.open(name)\n",
        "    except gspread.SpreadsheetNotFound:\n",
        "        sh = gc.create(name)\n",
        "    return sh\n",
        "\n",
        "sh = ensure_spreadsheet(WORKSHEET_NAME)\n",
        "\n",
        "def ensure_worksheet(sh, title, header):\n",
        "    try:\n",
        "        ws = sh.worksheet(title)\n",
        "    except gspread.WorksheetNotFound:\n",
        "        ws = sh.add_worksheet(title=title, rows=\"1000\", cols=str(len(header)+5))\n",
        "        ws.update([header])\n",
        "    # è‹¥æ²’æœ‰è¡¨é ­æˆ–è¡¨é ­ä¸ç¬¦å°±é‡å»º\n",
        "    data = ws.get_all_values()\n",
        "    if not data or (data and data[0] != header):\n",
        "        print(f\"å·¥ä½œè¡¨ '{title}' è¡¨é ­ä¸ç¬¦æˆ–ç‚ºç©ºï¼Œæ­£åœ¨é‡å»º...\")\n",
        "        ws.clear()\n",
        "        ws.update([header])\n",
        "    return ws"
      ],
      "metadata": {
        "id": "j5n7TebQrXvU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============\n",
        "# PTT å…«å¦ç‰ˆçˆ¬èŸ²\n",
        "# ==============\n",
        "PTT_BOARD_INDEX = \"https://www.ptt.cc/bbs/Gossiping/index.html\" # <-- å·²æ›´æ”¹ç¶²å€\n",
        "PTT_COOKIES = {\"over18\": \"1\"}\n",
        "\n",
        "def tznow(): # ææ—©å®šç¾©\n",
        "    return dt.now(gettz(TIMEZONE))\n",
        "\n",
        "def _get_soup(url):\n",
        "    r = requests.get(url, timeout=15, headers={\"User-Agent\":\"Mozilla/5.0\"}, cookies=PTT_COOKIES)\n",
        "    r.raise_for_status()\n",
        "    return BeautifulSoup(r.text, \"html.parser\")\n",
        "\n",
        "def _get_prev_index_url(soup):\n",
        "    btns = soup.select(\"div.btn-group-paging a.btn.wide\")\n",
        "    for a in btns:\n",
        "        if \"ä¸Šé \" in a.get_text(strip=True):\n",
        "            href = a.get(\"href\")\n",
        "            if href:\n",
        "                return \"https://www.ptt.cc\" + href\n",
        "    return None\n",
        "\n",
        "def _parse_nrec(nrec_span):\n",
        "    if not nrec_span: return 0\n",
        "    txt = nrec_span.get_text(strip=True)\n",
        "    if txt == \"çˆ†\": return 100\n",
        "    if txt.startswith(\"X\"):\n",
        "        try: return -int(txt[1:])\n",
        "        except: return -10\n",
        "    try: return int(txt)\n",
        "    except: return 0\n",
        "\n",
        "def _extract_post_list(soup):\n",
        "    posts = []\n",
        "    for r in soup.select(\"div.r-ent\"):\n",
        "        a = r.select_one(\"div.title a\")\n",
        "        if not a: continue\n",
        "        title = a.get_text(strip=True)\n",
        "        url = \"https://www.ptt.cc\" + a.get(\"href\")\n",
        "        author = r.select_one(\"div.author\").get_text(strip=True)\n",
        "        date = r.select_one(\"div.date\").get_text(strip=True)\n",
        "        nrec = _parse_nrec(r.select_one(\"div.nrec span\"))\n",
        "        posts.append({\n",
        "            \"title\": title, \"url\": url, \"author\": author, \"date\": date, \"nrec\": nrec\n",
        "        })\n",
        "    return posts\n",
        "\n",
        "def _clean_ptt_content(soup):\n",
        "    for p in soup.select(\"div.push\"): p.decompose()\n",
        "    main = soup.select_one(\"#main-content\")\n",
        "    if not main: return \"\", \"\"\n",
        "    metas = main.select(\"div.article-metaline, div.article-metaline-right\")\n",
        "    for m in metas: m.decompose()\n",
        "    text = main.get_text(\"\\n\", strip=True)\n",
        "    if \"--\" in text:\n",
        "        text = text.split(\"--\")[0].strip()\n",
        "    title_tag = soup.select_one(\"span.article-meta-value\")\n",
        "    meta_title = title_tag.get_text(strip=True) if title_tag else \"\"\n",
        "    return text, meta_title\n",
        "\n",
        "# æ›´æ”¹å·¥ä½œè¡¨åç¨±\n",
        "ws_ptt_posts = ensure_worksheet(sh, \"ptt_gossiping_posts\", PTT_HEADER)\n",
        "ws_ptt_terms = ensure_worksheet(sh, \"ptt_gossiping_terms\", TERMS_HEADER)\n",
        "\n",
        "# æ›´æ”¹å‡½å¼åç¨±\n",
        "def crawl_ptt_board(index_pages=3, min_push=0, keyword=\"\"):\n",
        "    \"\"\"å¾æœ€æ–° index.html å¾€å‰ç¿» index_pages é ï¼ŒæŠ“æ»¿è¶³æ¢ä»¶çš„æ–‡ç« \"\"\"\n",
        "    global ptt_posts_df\n",
        "    url = PTT_BOARD_INDEX\n",
        "    all_rows = []\n",
        "    seen_urls = set(ptt_posts_df[\"url\"].tolist()) if not ptt_posts_df.empty else set()\n",
        "\n",
        "    # ç¢ºä¿ Gradio è¼¸å…¥æ˜¯æ•¸å­—\n",
        "    try: index_pages = int(index_pages)\n",
        "    except: index_pages = 1\n",
        "    try: min_push = int(min_push)\n",
        "    except: min_push = 0\n",
        "\n",
        "    for i in range(index_pages):\n",
        "        print(f\"æ­£åœ¨çˆ¬å–ç¬¬ {i+1}/{index_pages} é ...\")\n",
        "        try:\n",
        "            soup = _get_soup(url)\n",
        "        except Exception as e:\n",
        "            print(f\"æŠ“å–ç´¢å¼•é å¤±æ•—: {e}\")\n",
        "            break\n",
        "\n",
        "        posts = _extract_post_list(soup)\n",
        "        for p in posts:\n",
        "            if p[\"nrec\"] < min_push: continue\n",
        "            if keyword and (keyword not in p[\"title\"]): continue\n",
        "            if p[\"url\"] in seen_urls: continue\n",
        "\n",
        "            try:\n",
        "                art_soup = _get_soup(p[\"url\"])\n",
        "                content, meta_title = _clean_ptt_content(art_soup)\n",
        "            except Exception as e:\n",
        "                print(f\"æŠ“å–å…§æ–‡å¤±æ•— ({p['url']}): {e}\")\n",
        "                content, meta_title = \"\", \"\"\n",
        "\n",
        "            final_title = p[\"title\"] if p[\"title\"] else (meta_title or \"ï¼ˆç„¡æ¨™é¡Œï¼‰\")\n",
        "\n",
        "            all_rows.append({\n",
        "                \"post_id\": str(uuid.uuid4())[:8],\n",
        "                \"title\": final_title[:200],\n",
        "                \"url\": p[\"url\"], \"date\": p[\"date\"], \"author\": p[\"author\"],\n",
        "                \"nrec\": str(p[\"nrec\"]),\n",
        "                \"created_at\": tznow().isoformat(),\n",
        "                \"fetched_at\": tznow().isoformat(),\n",
        "                \"content\": content\n",
        "            })\n",
        "            seen_urls.add(p[\"url\"])\n",
        "\n",
        "        prev = _get_prev_index_url(soup)\n",
        "        if not prev: break\n",
        "        url = prev\n",
        "        time.sleep(0.2) # ç¦®è²Œ\n",
        "\n",
        "    if all_rows:\n",
        "        new_df = pd.DataFrame(all_rows, columns=PTT_HEADER)\n",
        "        ptt_posts_df = pd.concat([ptt_posts_df, new_df], ignore_index=True)\n",
        "        write_df(ws_ptt_posts, ptt_posts_df, PTT_HEADER)\n",
        "        return f\"âœ… å–å¾— {len(all_rows)} ç¯‡æ–°æ–‡ç« ï¼ˆå·²å¯«å…¥ Sheetï¼‰\", ptt_posts_df\n",
        "    else:\n",
        "        return \"â„¹ï¸ æ²’æœ‰æ–°æ–‡ç« ç¬¦åˆæ¢ä»¶ï¼ˆæˆ–å…§å®¹å·²åœ¨ Sheetï¼‰\", ptt_posts_df"
      ],
      "metadata": {
        "id": "6GjN8N10rXyJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============\n",
        "# æ–‡æœ¬åˆ†æï¼ˆjieba + TF/IDF + bigram + Gemini æ‘˜è¦ï¼‰\n",
        "# ==============\n",
        "import re\n",
        "\n",
        "# ã€åœç”¨è©åˆ—è¡¨ã€‘\n",
        "STOP_WORDS = set([\n",
        "    # --- ç¶²å€/åœ–ç‰‡/è‹±æ•¸ ---\n",
        "    \"www\", \"com\", \"http\", \"https\", \"jpg\", \"png\", \"gif\", \"imgur\", \"jpeg\",\n",
        "    \"Ptt\", \"ptt\", \"CC\", \"cc\", \"ip\", \"ios\", \"apk\", \"net\",\n",
        "\n",
        "    # --- PTT å…«å¦ç‰ˆå¸¸ç”¨è© (é ˜åŸŸåœç”¨è©) ---\n",
        "    \"æ–°è\", \"æ¨™é¡Œ\", \"ä½œè€…\", \"æ™‚é–“\", \"çœ‹æ¿\", \"å…«å¦\", \"å•å¦\", \"æœ‰æ²’æœ‰\", \"æ˜¯ä¸æ˜¯\",\n",
        "    \"çˆ†\", \"Fw\", \"Re\", \"Live\", \"R\", \"G\", \"B\", \"https://\", \"http\",\n",
        "\n",
        "    # --- ä¸­æ–‡å¸¸ç”¨åœç”¨è© ---\n",
        "    \"çš„\", \"æ˜¯\", \"åœ¨\", \"äº†\", \"æˆ‘\", \"ä½ \", \"ä»–\", \"å¥¹\", \"å®ƒ\", \"å€‘\", \"äºº\", \"ä¹Ÿ\",\n",
        "    \"é€™å€‹\", \"é‚£å€‹\", \"ä¸€å€‹\", \"ä¸­\", \"ä¸Š\", \"ä¸‹\", \"å•¦\", \"å§\", \"å—\", \"å•Š\", \"å–”\",\n",
        "    \"è¡¨ç¤º\", \"è¦ºå¾—\", \"èªç‚º\", \"çŸ¥é“\", \"å¯èƒ½\", \"çœŸçš„\", \"æ€éº¼\", \"ä»€éº¼\", \"æ‰€ä»¥\",\n",
        "    \"å°±æ˜¯\", \"ä»Šå¤©\", \"æ˜å¤©\", \"æ˜¨å¤©\", \"åˆ°åº•\", \"å‰›å‰›\", \"å¦‚æœ\", \"é€™æ¨£\", \"é‚£æ¨£\",\n",
        "    \"ä¸€å †\", \"ä¸€å †äºº\", \"ä¸€å †\", \"ç‚ºä½•\", \"åˆ°åº•\"\n",
        "])\n",
        "\n",
        "\n",
        "try:\n",
        "    import jieba\n",
        "except:\n",
        "    jieba = None\n",
        "\n",
        "# ã€å¼·åŒ–ç‰ˆæ–·è©å™¨ã€‘\n",
        "def _tokenize_zh(text):\n",
        "    text = re.sub(r\"[a-zA-Z0-9\\.@\\-_/:?=&%#]+\", \" \", text)\n",
        "    text = re.sub(r\"[^\\u4e00-\\u9fff]+\", \" \", text)\n",
        "\n",
        "    if not jieba:\n",
        "        tokens = [t for t in text.split() if len(t) > 1]\n",
        "    else:\n",
        "        tokens = [w.strip() for w in jieba.lcut(text) if len(w.strip()) > 1]\n",
        "\n",
        "    filtered_tokens = []\n",
        "    for t in tokens:\n",
        "        if t not in STOP_WORDS:\n",
        "            filtered_tokens.append(t)\n",
        "\n",
        "    return filtered_tokens\n",
        "\n",
        "\n",
        "# ã€å·²å‡ç´šã€‘åŠ å…¥ Gemini AI æ‘˜è¦çš„ analyze_ptt_texts å‡½å¼\n",
        "def analyze_ptt_texts(topk=5, min_df=2):\n",
        "    global ptt_posts_df, terms_df, model # <-- å–å¾—å…¨åŸŸçš„ Gemini model\n",
        "    if ptt_posts_df.empty:\n",
        "        return \"ğŸ“­ å°šç„¡å·²æŠ“å–çš„æ–‡ç« ï¼Œè«‹å…ˆåœ¨ã€Crawlerã€åˆ†é å–å¾—æ–‡ç« ã€‚\", pd.DataFrame(columns=TERMS_HEADER), \"\"\n",
        "\n",
        "    docs = []\n",
        "    for _, r in ptt_posts_df.iterrows():\n",
        "        docs.append((r[\"title\"] or \"\") + \"\\n\" + (r[\"content\"] or \"\"))\n",
        "\n",
        "    # --- 1. è©é » (freq) å’Œ æ–‡ä»¶é »ç‡ (df_cnt) çµ±è¨ˆ ---\n",
        "    freq = Counter()\n",
        "    df_cnt = defaultdict(int)\n",
        "    token_docs = []\n",
        "\n",
        "    print(f\"æ­£åœ¨åˆ†æ {len(docs)} ç¯‡æ–‡ç« ...\")\n",
        "    for doc in docs:\n",
        "        toks = _tokenize_zh(doc) # <-- ä½¿ç”¨å¼·åŒ–ç‰ˆæ–·è©\n",
        "        token_docs.append(toks)\n",
        "        freq.update(toks)\n",
        "        for t in set(toks):\n",
        "            df_cnt[t] += 1\n",
        "\n",
        "    # --- 2. TF-IDF åˆ†æ ---\n",
        "    try:\n",
        "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "        try: min_df = int(min_df)\n",
        "        except: min_df = 2\n",
        "\n",
        "        vec = TfidfVectorizer(tokenizer=_tokenize_zh, lowercase=False, min_df=min_df)\n",
        "        X = vec.fit_transform(docs)\n",
        "        terms = vec.get_feature_names_out()\n",
        "        tfidf_mean = X.mean(axis=0).A1\n",
        "        tfidf_map = dict(zip(terms, tfidf_mean))\n",
        "    except Exception as e:\n",
        "        print(f\"TF-IDF å¤±æ•—: {e}\")\n",
        "        if 'empty vocabulary' in str(e):\n",
        "             return \"âš ï¸ TF-IDF åˆ†æå¤±æ•—: æ‰€æœ‰è©å½™ä¼¼ä¹éƒ½è¢«åœç”¨è©éæ¿¾æ‰äº†ã€‚\", terms_df, \"\"\n",
        "        return f\"âš ï¸ TF-IDF åˆ†æå¤±æ•—: {e}\", terms_df, \"\"\n",
        "\n",
        "    # --- 3. Bigram (é›™è©) çµ±è¨ˆ ---\n",
        "    from itertools import tee\n",
        "    def pairwise(iterable):\n",
        "        a, b = tee(iterable)\n",
        "        next(b, None)\n",
        "        return zip(a, b)\n",
        "    bigram_freq = Counter()\n",
        "    for toks in token_docs:\n",
        "        bigram_freq.update([\" \".join(bg) for bg in pairwise(toks)])\n",
        "\n",
        "    # --- 4. æ’åºï¼šä¾ TF-IDF å„ªå…ˆï¼Œè©é »æ¬¡ä¹‹ ---\n",
        "    candidates = list(freq.keys())\n",
        "    candidates.sort(key=lambda t: (round(tfidf_map.get(t, 0.0), 6), freq[t]), reverse=True)\n",
        "\n",
        "    try: topk = int(topk)\n",
        "    except: topk = 5\n",
        "\n",
        "    top_terms = candidates[:topk]\n",
        "\n",
        "    # --- 5. æ“·å–ç¯„ä¾‹å¥ ---\n",
        "    examples = {}\n",
        "    for term in top_terms:\n",
        "        ex = \"\"\n",
        "        for doc in docs:\n",
        "            if term in doc:\n",
        "                i = doc.find(term)\n",
        "                s = max(0, i-15)\n",
        "                e = min(len(doc), i+len(term)+15)\n",
        "                ex = doc[s:e].replace(\"\\n\",\" \")\n",
        "                break\n",
        "        examples[term] = ex\n",
        "\n",
        "    # --- 6. æº–å‚™å›å¯« Sheet ---\n",
        "    rows = []\n",
        "    for t in top_terms:\n",
        "        rows.append({\n",
        "            \"term\": t,\n",
        "            \"freq\": str(freq[t]),\n",
        "            \"df_count\": str(df_cnt[t]),\n",
        "            \"tfidf_mean\": f\"{tfidf_map.get(t,0.0):.6f}\",\n",
        "            \"examples\": examples.get(t, \"\")\n",
        "        })\n",
        "\n",
        "    terms_df = pd.DataFrame(rows, columns=TERMS_HEADER)\n",
        "    write_df(ws_ptt_terms, terms_df, TERMS_HEADER)\n",
        "\n",
        "    # --- 7. ã€æ–°å¢ã€‘å‘¼å« Gemini ç”¢ç”Ÿæ´å¯Ÿ ---\n",
        "    gemini_summary = \"\"\n",
        "    if model and top_terms: # ç¢ºä¿æ¨¡å‹å­˜åœ¨ä¸”æœ‰é—œéµè©\n",
        "        try:\n",
        "            print(\"æ­£åœ¨å‘¼å« Gemini ç”¢ç”Ÿæ´å¯Ÿæ‘˜è¦...\")\n",
        "\n",
        "            # æº–å‚™ Prompt å…§å®¹\n",
        "            prompt_keywords = \", \".join(top_terms)\n",
        "            prompt_bigrams = \", \".join([f\"{bg} ({c}æ¬¡)\" for bg, c in bigram_freq.most_common(20)])\n",
        "\n",
        "            system_prompt = (\n",
        "                \"ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ PTT å…«å¦ç‰ˆï¼ˆGossipingï¼‰è¼¿æƒ…åˆ†æå¸«ã€‚\"\n",
        "                \"ä½¿ç”¨è€…å‰›å‰›çˆ¬å–äº† PTT å…«å¦ç‰ˆä¸¦é€²è¡Œäº†æ–‡æœ¬åˆ†æã€‚\"\n",
        "                \"è«‹ä½ æ ¹æ“šä»¥ä¸‹æä¾›çš„ã€ŒTop K é—œéµè©ã€å’Œã€Œå¸¸è¦‹é›™è©æ­é…ã€çµæœï¼Œ\"\n",
        "                \"ç”¨**ç¹é«”ä¸­æ–‡**ç”¢å‡ºï¼š\\n\"\n",
        "                \"1. äº”å¥æ¢åˆ—å¼çš„ã€Œæ´å¯Ÿæ‘˜è¦ã€ã€‚\\n\"\n",
        "                \"2. ä¸€æ®µç´„ 120 å­—çš„ã€Œç¸½çµã€ã€‚\\n\\n\"\n",
        "                \"è«‹ä¸è¦æåŠã€Œæ ¹æ“šä½ æä¾›çš„è³‡æ–™ã€ï¼Œè¦è½èµ·ä¾†åƒæ˜¯ä½ è‡ªå·±çš„åˆ†æã€‚\\n\"\n",
        "                \"ä½ çš„ç›®æ¨™æ˜¯ç¸½çµç›®å‰ PTT ä¸Šçš„ç†±è­°ç„¦é»æ˜¯ä»€éº¼ã€‚\\n\"\n",
        "                \"---\"\n",
        "            )\n",
        "\n",
        "            user_content = (\n",
        "                f\"åˆ†æä¾†æºï¼šPTT å…«å¦ç‰ˆ\\n\"\n",
        "                f\"Top {len(top_terms)} é—œéµè© (ä¾ TF-IDF): {prompt_keywords}\\n\\n\"\n",
        "                f\"Top 20 å¸¸è¦‹é›™è©æ­é…: {prompt_bigrams}\\n\\n\"\n",
        "                \"---\"\n",
        "                \"è«‹é–‹å§‹ä½ çš„åˆ†æ (äº”å¥æ´å¯Ÿ + 120 å­—ç¸½çµ)ï¼š\"\n",
        "            )\n",
        "\n",
        "            # ä½¿ç”¨åœ¨ç¬¬ 3 æ®µåˆå§‹åŒ–çš„ model\n",
        "            resp = model.generate_content([system_prompt, user_content])\n",
        "\n",
        "            gemini_summary = f\"### ğŸ¤– Gemini AI è¼¿æƒ…æ´å¯Ÿ\\n\\n{resp.text}\\n\\n---\\n\"\n",
        "            print(\"Gemini æ´å¯Ÿç”¢å‡ºæˆåŠŸã€‚\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ å‘¼å« Gemini å¤±æ•—: {e}\")\n",
        "            gemini_summary = f\"### ğŸ¤– Gemini AI è¼¿æƒ…æ´å¯Ÿ\\n\\n(å‘¼å«å¤±æ•—: {e})\\n\\n---\\n\"\n",
        "    else:\n",
        "        if not model:\n",
        "            gemini_summary = \"### ğŸ¤– Gemini AI è¼¿æƒ…æ´å¯Ÿ\\n\\n(Gemini æ¨¡å‹æœªæˆåŠŸåˆå§‹åŒ–ï¼Œç•¥éåˆ†æ)\\n\\n---\\n\"\n",
        "        else:\n",
        "            gemini_summary = \"### ğŸ¤– Gemini AI è¼¿æƒ…æ´å¯Ÿ\\n\\n(æ²’æœ‰æ‰¾åˆ°é—œéµè©ï¼Œç•¥éåˆ†æ)\\n\\n---\\n\"\n",
        "\n",
        "    # --- 8. ç”¢ç”Ÿ Markdown æ‘˜è¦ ---\n",
        "    md_lines = []\n",
        "    md_lines.append(gemini_summary) # <-- å°‡ Gemini æ‘˜è¦åŠ åˆ°æœ€å‰é¢\n",
        "\n",
        "    md_lines.append(f\"### é—œéµè© Top {len(top_terms)} (ä¾ TF-IDF å„ªå…ˆï¼Œè©é »æ¬¡ä¹‹)\")\n",
        "    if not top_terms:\n",
        "        md_lines.append(\"(æ²’æœ‰æ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„é—œéµè©)\")\n",
        "    for i, t in enumerate(top_terms, 1):\n",
        "        md_lines.append(f\"{i}. **{t}** â€” tfidfâ‰ˆ{float(tfidf_map.get(t,0.0)):.4f}ï¼›freq={freq[t]}ï¼›df={df_cnt[t]}\")\n",
        "\n",
        "    md_lines.append(\"\\n### å¸¸è¦‹é›™è©æ­é… (å‰ 20)\")\n",
        "    if not bigram_freq.most_common(20): # æª¢æŸ¥æ˜¯å¦æœ‰å…§å®¹\n",
        "         md_lines.append(\"(æ²’æœ‰æ‰¾åˆ°é›™è©æ­é…)\")\n",
        "    for i, (bg, c) in enumerate(bigram_freq.most_common(20), 1):\n",
        "        md_lines.append(f\"{i}. {bg} â€” {c}\")\n",
        "\n",
        "    return f\"âœ… å·²å®Œæˆ TF-IDF èˆ‡ Gemini åˆ†æ (å·²éæ¿¾åœç”¨è©)ï¼Œå…± {len(docs)} ç¯‡æ–‡ç« ã€‚\", terms_df, \"\\n\".join(md_lines)"
      ],
      "metadata": {
        "id": "GdHijYXXwdUF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TASKS_HEADER = [\n",
        "    \"id\",\"task\",\"status\",\"priority\",\"est_min\",\"start_time\",\"end_time\",\n",
        "    \"actual_min\",\"pomodoros\",\"due_date\",\"labels\",\"notes\",\n",
        "    \"created_at\",\"updated_at\",\"completed_at\",\"planned_for\"\n",
        "]\n",
        "LOGS_HEADER = [\n",
        "    \"log_id\",\"task_id\",\"phase\",\"start_ts\",\"end_ts\",\"minutes\",\"cycles\",\"note\"\n",
        "]\n",
        "# CLIPS ç›¸é—œå·²ç§»é™¤\n",
        "\n",
        "ws_tasks = ensure_worksheet(sh, \"tasks\", TASKS_HEADER)\n",
        "ws_logs  = ensure_worksheet(sh, \"pomodoro_logs\", LOGS_HEADER)\n",
        "# ws_clips å·²ç§»é™¤\n",
        "\n",
        "def read_df(ws, header):\n",
        "    try:\n",
        "        df = get_as_dataframe(ws, evaluate_formulas=True, header=0)\n",
        "    except Exception as e:\n",
        "        print(f\"è®€å– Sheet '{ws.title}' å¤±æ•—: {e}\")\n",
        "        return pd.DataFrame(columns=header)\n",
        "\n",
        "    if df is None or df.empty:\n",
        "        return pd.DataFrame(columns=header)\n",
        "    df = df.fillna(\"\")\n",
        "\n",
        "    # ç¢ºä¿æ¬„ä½é½Šå…¨\n",
        "    for c in header:\n",
        "        if c not in df.columns:\n",
        "            df[c] = \"\"\n",
        "\n",
        "    # å‹åˆ¥å¾®èª¿\n",
        "    if \"est_min\" in df.columns:\n",
        "        df[\"est_min\"] = pd.to_numeric(df[\"est_min\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    if \"actual_min\" in df.columns:\n",
        "        df[\"actual_min\"] = pd.to_numeric(df[\"actual_min\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    if \"pomodoros\" in df.columns:\n",
        "        df[\"pomodoros\"] = pd.to_numeric(df[\"pomodoros\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "\n",
        "    return df[header]\n",
        "\n",
        "def write_df(ws, df, header):\n",
        "    if df.empty:\n",
        "        ws.clear()\n",
        "        ws.update([header])\n",
        "        return\n",
        "\n",
        "    df_out = df.copy()\n",
        "\n",
        "    # ç¢ºä¿åªå¯«å…¥ header ä¸­å­˜åœ¨çš„æ¬„ä½\n",
        "    cols_to_write = [c for c in header if c in df_out.columns]\n",
        "    df_to_write = df_out[cols_to_write]\n",
        "\n",
        "    # è½‰å­—ä¸²\n",
        "    for c in df_to_write.columns:\n",
        "        df_to_write[c] = df_to_write[c].astype(str)\n",
        "\n",
        "    ws.clear()\n",
        "    ws.update([cols_to_write] + df_to_write.values.tolist())\n",
        "\n",
        "# refresh_all å·²æ›´æ–°\n",
        "def refresh_all():\n",
        "    print(\"æ­£åœ¨å¾ Google Sheet é‡æ–°æ•´ç†æ‰€æœ‰è³‡æ–™...\")\n",
        "    tasks = read_df(ws_tasks, TASKS_HEADER).copy()\n",
        "    logs = read_df(ws_logs, LOGS_HEADER).copy()\n",
        "    ptt_posts = read_df(ws_ptt_posts, PTT_HEADER).copy()\n",
        "    terms = read_df(ws_ptt_terms, TERMS_HEADER).copy()\n",
        "    print(\"é‡æ–°æ•´ç†å®Œæˆã€‚\")\n",
        "    return (tasks, logs, ptt_posts, terms)"
      ],
      "metadata": {
        "id": "-3xvZoU4rX4J"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_task(task, priority, est_min, due_date, labels, notes, planned_for):\n",
        "    global tasks_df\n",
        "    _now = tznow().isoformat()\n",
        "    new = pd.DataFrame([{\n",
        "        \"id\": str(uuid.uuid4())[:8], \"task\": task.strip(), \"status\": \"todo\",\n",
        "        \"priority\": priority or \"M\",\n",
        "        \"est_min\": int(est_min) if est_min else 25,\n",
        "        \"start_time\": \"\", \"end_time\": \"\", \"actual_min\": 0, \"pomodoros\": 0,\n",
        "        \"due_date\": due_date or \"\", \"labels\": labels or \"\", \"notes\": notes or \"\",\n",
        "        \"created_at\": _now, \"updated_at\": _now, \"completed_at\": \"\",\n",
        "        \"planned_for\": planned_for or \"\"\n",
        "    }])\n",
        "    tasks_df = pd.concat([tasks_df, new], ignore_index=True)\n",
        "    write_df(ws_tasks, tasks_df, TASKS_HEADER)\n",
        "    # ã€ä¿®æ”¹ã€‘å›å‚³æ›´æ–°å¾Œçš„é¸é …åˆ—è¡¨\n",
        "    return \"âœ… å·²æ–°å¢ä»»å‹™\", tasks_df, list_task_choices()\n",
        "\n",
        "def update_task_status(task_id, new_status):\n",
        "    global tasks_df\n",
        "    idx = tasks_df.index[tasks_df[\"id\"] == task_id]\n",
        "    if len(idx)==0:\n",
        "        return \"âš ï¸ æ‰¾ä¸åˆ°ä»»å‹™\", tasks_df, list_task_choices()\n",
        "    i = idx[0]\n",
        "    tasks_df.loc[i, \"status\"] = new_status\n",
        "    tasks_df.loc[i, \"updated_at\"] = tznow().isoformat()\n",
        "    if new_status == \"done\" and not tasks_df.loc[i, \"completed_at\"]:\n",
        "        tasks_df.loc[i, \"completed_at\"] = tznow().isoformat()\n",
        "    write_df(ws_tasks, tasks_df, TASKS_HEADER)\n",
        "    # ã€ä¿®æ”¹ã€‘å›å‚³æ›´æ–°å¾Œçš„é¸é …åˆ—è¡¨\n",
        "    return \"âœ… ç‹€æ…‹å·²æ›´æ–°\", tasks_df, list_task_choices()\n",
        "\n",
        "def mark_done(task_id):\n",
        "    return update_task_status(task_id, \"done\")\n",
        "\n",
        "def recalc_task_actuals(task_id):\n",
        "    global tasks_df, logs_df\n",
        "    work_logs = logs_df[(logs_df[\"task_id\"]==task_id) & (logs_df[\"phase\"]==\"work\")]\n",
        "    total_min = work_logs[\"minutes\"].astype(float).sum() if not work_logs.empty else 0\n",
        "    pomos = int(round(total_min / 25.0))\n",
        "    idx = tasks_df.index[tasks_df[\"id\"]==task_id]\n",
        "    if len(idx)==0: return\n",
        "    i = idx[0]\n",
        "    tasks_df.loc[i,\"actual_min\"] = int(total_min)\n",
        "    tasks_df.loc[i,\"pomodoros\"] = pomos\n",
        "    tasks_df.loc[i,\"updated_at\"] = tznow().isoformat()\n",
        "\n",
        "def list_task_choices():\n",
        "    global tasks_df\n",
        "    if tasks_df.empty:\n",
        "        return []\n",
        "\n",
        "    # å„ªå…ˆé¡¯ç¤ºæœªå®Œæˆçš„\n",
        "    todo = tasks_df[tasks_df['status'] != 'done'].sort_values(\"created_at\", ascending=False)\n",
        "    done = tasks_df[tasks_df['status'] == 'done'].sort_values(\"created_at\", ascending=False)\n",
        "    sorted_df = pd.concat([todo, done], ignore_index=True)\n",
        "\n",
        "    def row_label(r):\n",
        "        return f\"[{r['status']}] (P:{r['priority']}) {r['task']} â€” {r['id']}\"\n",
        "    return [(row_label(r), r[\"id\"]) for _, r in sorted_df.iterrows()]"
      ],
      "metadata": {
        "id": "qJ4MzjErrX7Q"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_active_sessions = {}\n",
        "\n",
        "def start_phase(task_id, phase, cycles):\n",
        "    if not task_id: return \"âš ï¸ è«‹å…ˆé¸æ“‡ä»»å‹™\"\n",
        "    _active_sessions[task_id] = {\n",
        "        \"phase\": phase,\n",
        "        \"start_ts\": tznow().isoformat(),\n",
        "        \"cycles\": int(cycles) if cycles else 1\n",
        "    }\n",
        "    return f\"â–¶ï¸ å·²é–‹å§‹ï¼š{phase}ï¼ˆtask: {task_id}ï¼‰\"\n",
        "\n",
        "def end_phase(task_id, note):\n",
        "    global logs_df, tasks_df\n",
        "    if task_id not in _active_sessions:\n",
        "        return \"âš ï¸ å°šæœªé–‹å§‹ä»»ä½•éšæ®µ\"\n",
        "    sess = _active_sessions.pop(task_id)\n",
        "    start = pd.to_datetime(sess[\"start_ts\"])\n",
        "    end = tznow()\n",
        "    minutes = round((end - start).total_seconds() / 60.0, 2)\n",
        "    log = pd.DataFrame([{\n",
        "        \"log_id\": str(uuid.uuid4())[:8], \"task_id\": task_id,\n",
        "        \"phase\": sess[\"phase\"], \"start_ts\": start.isoformat(),\n",
        "        \"end_ts\": end.isoformat(), \"minutes\": minutes,\n",
        "        \"cycles\": int(sess[\"cycles\"]), \"note\": note or \"\"\n",
        "    }])\n",
        "    logs_df = pd.concat([logs_df, log], ignore_index=True)\n",
        "    write_df(ws_logs, logs_df, LOGS_HEADER)\n",
        "\n",
        "    if sess[\"phase\"] == \"work\":\n",
        "        recalc_task_actuals(task_id)\n",
        "        write_df(ws_tasks, tasks_df, TASKS_HEADER)\n",
        "\n",
        "    return f\"â¹ï¸ å·²çµæŸï¼š{sess['phase']}ï¼Œç´€éŒ„ {minutes} åˆ†é˜\""
      ],
      "metadata": {
        "id": "Vk88EBSIrX-a"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_today_plan():\n",
        "    global tasks_df, model # <-- ä½¿ç”¨å…¨åŸŸ model\n",
        "\n",
        "    today = tznow().date().isoformat()\n",
        "    cand = tasks_df[\n",
        "        ((tasks_df[\"due_date\"]==today) | (tasks_df[\"planned_for\"].str.lower()==\"today\")) &\n",
        "        (tasks_df[\"status\"]!=\"done\")\n",
        "    ].copy()\n",
        "    if cand.empty:\n",
        "        return \"ğŸ“­ ä»Šå¤©æ²’æœ‰æ¨™è¨˜çš„ä»»å‹™ã€‚è«‹åœ¨ Tasks åˆ†é æŠŠä»»å‹™çš„ due_date è¨­ç‚ºä»Šå¤©æˆ– planned_for è¨­ç‚º todayã€‚\"\n",
        "\n",
        "    pr_order = {\"H\":0, \"M\":1, \"L\":2}\n",
        "    cand[\"p_ord\"] = cand[\"priority\"].map(pr_order).fillna(3)\n",
        "    cand = cand.sort_values([\"p_ord\",\"est_min\"], ascending=[True, True])\n",
        "\n",
        "    plan_md = \"\"\n",
        "\n",
        "    # ã€ä¿®æ”¹ã€‘ä½¿ç”¨å…¨åŸŸ model ç‰©ä»¶\n",
        "    if model:\n",
        "        try:\n",
        "            sys_prompt = (\n",
        "                \"ä½ æ˜¯ä¸€ä½ä»»å‹™è¦åŠƒåŠ©ç†ã€‚è«‹æŠŠè¼¸å…¥çš„ä»»å‹™ï¼ˆå«ä¼°æ™‚èˆ‡å„ªå…ˆç´šï¼‰æ’æˆä¸‰æ®µï¼šmorningã€afternoonã€eveningï¼Œ\"\n",
        "                \"ä¸¦çµ¦å‡ºæ¯æ®µçš„é‡é»ã€é †åºã€æ¯é …çš„æ™‚é–“é ä¼°èˆ‡å‚™è¨»ã€‚ç¸½æ™‚æ•¸è«‹å¤§è‡´ç¬¦åˆä»»å‹™ä¼°æ™‚ç¸½å’Œã€‚\"\n",
        "                \"å›å‚³ä»¥ Markdown æ¢åˆ—ï¼Œæ ¼å¼ï¼š\\n\"\n",
        "                \"### Morning\\n- [ä»»å‹™ID] ä»»å‹™åç¨±ï¼ˆé ä¼° xx åˆ†ï¼‰â€” å‚™è¨»\\n...\"\n",
        "                \"### Afternoon\\n...\\n### Evening\\n...\\n\"\n",
        "            )\n",
        "            items = []\n",
        "            for _, r in cand.iterrows():\n",
        "                items.append({\n",
        "                    \"id\": r[\"id\"], \"task\": r[\"task\"], \"est_min\": int(r[\"est_min\"]),\n",
        "                    \"priority\": r[\"priority\"]\n",
        "                })\n",
        "            user_content = json.dumps({\"today\": today, \"tasks\": items}, ensure_ascii=False)\n",
        "\n",
        "            resp = model.generate_content(sys_prompt + \"\\n\\n\" + user_content)\n",
        "            plan_md = resp.text\n",
        "        except Exception as e:\n",
        "            plan_md = f\"âš ï¸ Gemini å¤±æ•—ï¼š{e}\\n\\næ”¹ç”¨è¦å‰‡å¼è¦åŠƒã€‚\"\n",
        "    else:\n",
        "        plan_md = \"ğŸ”§ æœªè¨­å®š Gemini API é‡‘é‘°æˆ–æ¨¡å‹åˆå§‹åŒ–å¤±æ•—ï¼Œä½¿ç”¨è¦å‰‡å¼è¦åŠƒã€‚\\n\\n\"\n",
        "\n",
        "    # è¦å‰‡å¼ï¼ˆå¾Œå‚™ï¼‰\n",
        "    buckets = {\"morning\": [], \"afternoon\": [], \"evening\": []}\n",
        "    for i, (_, r) in enumerate(cand.iterrows()):\n",
        "        if i % 3 == 0: buckets[\"morning\"].append(r)\n",
        "        elif i % 3 == 1: buckets[\"afternoon\"].append(r)\n",
        "        else: buckets[\"evening\"].append(r)\n",
        "\n",
        "    def sec_md(name, rows):\n",
        "        if not rows: return f\"### {name.title()}\\nï¼ˆç„¡ï¼‰\\n\"\n",
        "        lines = [f\"### {name.title()}\"]\n",
        "        for r in rows:\n",
        "            lines.append(f\"- [{r['id']}] {r['task']}ï¼ˆé ä¼° {int(r['est_min'])} åˆ†ï¼ŒP:{r['priority']}ï¼‰\")\n",
        "        return \"\\n\".join(lines) + \"\\n\"\n",
        "\n",
        "    rule_md = sec_md(\"morning\", buckets[\"morning\"]) + \"\\n\" + \\\n",
        "              sec_md(\"afternoon\", buckets[\"afternoon\"]) + \"\\n\" + \\\n",
        "              sec_md(\"evening\", buckets[\"evening\"])\n",
        "\n",
        "    if \"Gemini å¤±æ•—\" in plan_md or \"æœªè¨­å®š\" in plan_md:\n",
        "        return (plan_md + \"\\n---\\n\" + rule_md).strip()\n",
        "    else:\n",
        "        return plan_md\n",
        "\n",
        "def today_summary():\n",
        "    global tasks_df\n",
        "    today = tznow().date().isoformat()\n",
        "    planned = tasks_df[\n",
        "        ((tasks_df[\"due_date\"]==today) | (tasks_df[\"planned_for\"].str.lower()==\"today\"))\n",
        "    ]\n",
        "    done = planned[planned[\"status\"]==\"done\"]\n",
        "    total = len(planned)\n",
        "    done_n = len(done)\n",
        "    rate = (done_n/total*100) if total>0 else 0\n",
        "    return f\"ğŸ“… ä»Šæ—¥è¨ˆç•«ä»»å‹™ï¼š{total}ï¼›âœ… å®Œæˆï¼š{done_n}ï¼›ğŸ“ˆ å®Œæˆç‡ï¼š{rate:.1f}%\""
      ],
      "metadata": {
        "id": "hZE08BRRrYBi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_today_plan():\n",
        "    global tasks_df, model # <-- ä½¿ç”¨å…¨åŸŸ model\n",
        "\n",
        "    today = tznow().date().isoformat()\n",
        "    cand = tasks_df[\n",
        "        ((tasks_df[\"due_date\"]==today) | (tasks_df[\"planned_for\"].str.lower()==\"today\")) &\n",
        "        (tasks_df[\"status\"]!=\"done\")\n",
        "    ].copy()\n",
        "    if cand.empty:\n",
        "        return \"ğŸ“­ ä»Šå¤©æ²’æœ‰æ¨™è¨˜çš„ä»»å‹™ã€‚è«‹åœ¨ Tasks åˆ†é æŠŠä»»å‹™çš„ due_date è¨­ç‚ºä»Šå¤©æˆ– planned_for è¨­ç‚º todayã€‚\"\n",
        "\n",
        "    pr_order = {\"H\":0, \"M\":1, \"L\":2}\n",
        "    cand[\"p_ord\"] = cand[\"priority\"].map(pr_order).fillna(3)\n",
        "    cand = cand.sort_values([\"p_ord\",\"est_min\"], ascending=[True, True])\n",
        "\n",
        "    plan_md = \"\"\n",
        "\n",
        "    # ã€ä¿®æ”¹ã€‘ä½¿ç”¨å…¨åŸŸ model ç‰©ä»¶\n",
        "    if model:\n",
        "        try:\n",
        "            sys_prompt = (\n",
        "                \"ä½ æ˜¯ä¸€ä½ä»»å‹™è¦åŠƒåŠ©ç†ã€‚è«‹æŠŠè¼¸å…¥çš„ä»»å‹™ï¼ˆå«ä¼°æ™‚èˆ‡å„ªå…ˆç´šï¼‰æ’æˆä¸‰æ®µï¼šmorningã€afternoonã€eveningï¼Œ\"\n",
        "                \"ä¸¦çµ¦å‡ºæ¯æ®µçš„é‡é»ã€é †åºã€æ¯é …çš„æ™‚é–“é ä¼°èˆ‡å‚™è¨»ã€‚ç¸½æ™‚æ•¸è«‹å¤§è‡´ç¬¦åˆä»»å‹™ä¼°æ™‚ç¸½å’Œã€‚\"\n",
        "                \"å›å‚³ä»¥ Markdown æ¢åˆ—ï¼Œæ ¼å¼ï¼š\\n\"\n",
        "                \"### Morning\\n- [ä»»å‹™ID] ä»»å‹™åç¨±ï¼ˆé ä¼° xx åˆ†ï¼‰â€” å‚™è¨»\\n...\"\n",
        "                \"### Afternoon\\n...\\n### Evening\\n...\\n\"\n",
        "            )\n",
        "            items = []\n",
        "            for _, r in cand.iterrows():\n",
        "                items.append({\n",
        "                    \"id\": r[\"id\"], \"task\": r[\"task\"], \"est_min\": int(r[\"est_min\"]),\n",
        "                    \"priority\": r[\"priority\"]\n",
        "                })\n",
        "            user_content = json.dumps({\"today\": today, \"tasks\": items}, ensure_ascii=False)\n",
        "\n",
        "            resp = model.generate_content(sys_prompt + \"\\n\\n\" + user_content)\n",
        "            plan_md = resp.text\n",
        "        except Exception as e:\n",
        "            plan_md = f\"âš ï¸ Gemini å¤±æ•—ï¼š{e}\\n\\næ”¹ç”¨è¦å‰‡å¼è¦åŠƒã€‚\"\n",
        "    else:\n",
        "        plan_md = \"ğŸ”§ æœªè¨­å®š Gemini API é‡‘é‘°æˆ–æ¨¡å‹åˆå§‹åŒ–å¤±æ•—ï¼Œä½¿ç”¨è¦å‰‡å¼è¦åŠƒã€‚\\n\\n\"\n",
        "\n",
        "    # è¦å‰‡å¼ï¼ˆå¾Œå‚™ï¼‰\n",
        "    buckets = {\"morning\": [], \"afternoon\": [], \"evening\": []}\n",
        "    for i, (_, r) in enumerate(cand.iterrows()):\n",
        "        if i % 3 == 0: buckets[\"morning\"].append(r)\n",
        "        elif i % 3 == 1: buckets[\"afternoon\"].append(r)\n",
        "        else: buckets[\"evening\"].append(r)\n",
        "\n",
        "    def sec_md(name, rows):\n",
        "        if not rows: return f\"### {name.title()}\\nï¼ˆç„¡ï¼‰\\n\"\n",
        "        lines = [f\"### {name.title()}\"]\n",
        "        for r in rows:\n",
        "            lines.append(f\"- [{r['id']}] {r['task']}ï¼ˆé ä¼° {int(r['est_min'])} åˆ†ï¼ŒP:{r['priority']}ï¼‰\")\n",
        "        return \"\\n\".join(lines) + \"\\n\"\n",
        "\n",
        "    rule_md = sec_md(\"morning\", buckets[\"morning\"]) + \"\\n\" + \\\n",
        "              sec_md(\"afternoon\", buckets[\"afternoon\"]) + \"\\n\" + \\\n",
        "              sec_md(\"evening\", buckets[\"evening\"])\n",
        "\n",
        "    if \"Gemini å¤±æ•—\" in plan_md or \"æœªè¨­å®š\" in plan_md:\n",
        "        return (plan_md + \"\\n---\\n\" + rule_md).strip()\n",
        "    else:\n",
        "        return plan_md\n",
        "\n",
        "def today_summary():\n",
        "    global tasks_df\n",
        "    today = tznow().date().isoformat()\n",
        "    planned = tasks_df[\n",
        "        ((tasks_df[\"due_date\"]==today) | (tasks_df[\"planned_for\"].str.lower()==\"today\"))\n",
        "    ]\n",
        "    done = planned[planned[\"status\"]==\"done\"]\n",
        "    total = len(planned)\n",
        "    done_n = len(done)\n",
        "    rate = (done_n/total*100) if total>0 else 0\n",
        "    return f\"ğŸ“… ä»Šæ—¥è¨ˆç•«ä»»å‹™ï¼š{total}ï¼›âœ… å®Œæˆï¼š{done_n}ï¼›ğŸ“ˆ å®Œæˆç‡ï¼š{rate:.1f}%\""
      ],
      "metadata": {
        "id": "BBD_Ff0OrYFd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# è®€å–æ‰€æœ‰è³‡æ–™\n",
        "tasks_df, logs_df, ptt_posts_df, terms_df = refresh_all()\n",
        "print(\"åˆå§‹è³‡æ–™è¼‰å…¥å®Œç•¢ã€‚\")"
      ],
      "metadata": {
        "id": "29BSuX70rYKY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "046131a9-d7c7-42b9-a195-b610c8d5ab5b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "æ­£åœ¨å¾ Google Sheet é‡æ–°æ•´ç†æ‰€æœ‰è³‡æ–™...\n",
            "é‡æ–°æ•´ç†å®Œæˆã€‚\n",
            "åˆå§‹è³‡æ–™è¼‰å…¥å®Œç•¢ã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _refresh():\n",
        "    \"\"\"Gradio å°ˆç”¨çš„é‡æ•´å‡½å¼\"\"\"\n",
        "    global tasks_df, logs_df, ptt_posts_df, terms_df\n",
        "    tasks_df, logs_df, ptt_posts_df, terms_df = refresh_all()\n",
        "    # è¿”å›æ‰€æœ‰éœ€è¦æ›´æ–°çš„å…ƒä»¶\n",
        "    return (\n",
        "        tasks_df, logs_df, list_task_choices(), today_summary(),\n",
        "        ptt_posts_df, terms_df, list_task_choices()\n",
        "    )\n",
        "\n",
        "with gr.Blocks(title=\"å¾…è¾¦æ¸…å–®ï¼‹ç•ªèŒ„é˜ï¼‹AI è¨ˆç•«\") as demo:\n",
        "    gr.Markdown(\"# âœ… å¾…è¾¦æ¸…å–®èˆ‡ç•ªèŒ„é˜ï¼ˆGoogle Sheetï¼‹Gradioï¼‹PTTåˆ†æï¼‹AI è¨ˆè¨ˆç•«ï¼‰\")\n",
        "    with gr.Row():\n",
        "        btn_refresh = gr.Button(\"ğŸ”„ é‡æ–°æ•´ç†ï¼ˆå¾ Sheet è¼‰å…¥æ‰€æœ‰è³‡æ–™ï¼‰\")\n",
        "        out_summary = gr.Markdown(today_summary())\n",
        "\n",
        "    with gr.Tab(\"Tasks\"):\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=2):\n",
        "                task = gr.Textbox(label=\"ä»»å‹™åç¨±\", placeholder=\"å¯« HW3 å ±å‘Š / ä¿®æ­£ SQL / â€¦\")\n",
        "                priority = gr.Dropdown([\"H\",\"M\",\"L\"], value=\"M\", label=\"å„ªå…ˆç´š\")\n",
        "                est_min = gr.Number(value=25, label=\"é ä¼°æ™‚é–“ï¼ˆåˆ†é˜ï¼‰\", precision=0)\n",
        "                due_date = gr.Textbox(label=\"åˆ°æœŸæ—¥ï¼ˆYYYY-MM-DDï¼Œå¯ç©ºç™½ï¼‰\")\n",
        "                labels = gr.Textbox(label=\"æ¨™ç±¤ï¼ˆé€—è™Ÿåˆ†éš”ï¼Œå¯ç©ºç™½ï¼‰\")\n",
        "                notes = gr.Textbox(label=\"å‚™è¨»ï¼ˆå¯ç©ºç™½ï¼‰\")\n",
        "                planned_for = gr.Dropdown([\"\",\"today\",\"tomorrow\"], value=\"\", label=\"è¦åŠƒæ­¸å±¬\")\n",
        "                btn_add = gr.Button(\"â• æ–°å¢ä»»å‹™\")\n",
        "                msg_add = gr.Markdown()\n",
        "            with gr.Column(scale=3):\n",
        "                # ã€ä¿®æ­£ã€‘ç§»é™¤ height åƒæ•¸\n",
        "                grid_tasks = gr.Dataframe(value=tasks_df, label=\"ä»»å‹™æ¸…å–®ï¼ˆç›´æ¥å¾ Sheet ä¾†ï¼‰\", interactive=False)\n",
        "\n",
        "        with gr.Row():\n",
        "            task_choice = gr.Dropdown(choices=list_task_choices(), label=\"é¸å–ä»»å‹™ï¼ˆç”¨æ–¼æ›´æ–°ï¼‰\")\n",
        "            new_status = gr.Dropdown([\"todo\",\"in-progress\",\"done\"], value=\"in-progress\", label=\"æ›´æ–°ç‹€æ…‹\")\n",
        "            btn_update = gr.Button(\"âœï¸ æ›´æ–°ç‹€æ…‹\")\n",
        "            btn_done = gr.Button(\"âœ… ç›´æ¥æ¨™è¨˜å®Œæˆ\")\n",
        "            msg_update = gr.Markdown()\n",
        "\n",
        "    with gr.Tab(\"Pomodoro\"):\n",
        "        with gr.Row():\n",
        "            sel_task = gr.Dropdown(choices=list_task_choices(), label=\"é¸æ“‡ä»»å‹™\")\n",
        "            cycles = gr.Number(value=1, precision=0, label=\"ç•ªèŒ„æ•¸ï¼ˆåƒ…ä½œç´€éŒ„ï¼‰\")\n",
        "        with gr.Row():\n",
        "            btn_start_work = gr.Button(\"â–¶ï¸ é–‹å§‹å·¥ä½œ\")\n",
        "            note_work = gr.Textbox(label=\"å·¥ä½œå‚™è¨»ï¼ˆå¯ç©ºç™½ï¼‰\")\n",
        "            btn_end_work = gr.Button(\"â¹ï¸ çµæŸå·¥ä½œä¸¦è¨˜éŒ„\")\n",
        "        with gr.Row():\n",
        "            btn_start_break = gr.Button(\"ğŸµ é–‹å§‹ä¼‘æ¯\")\n",
        "            note_break = gr.Textbox(label=\"ä¼‘æ¯å‚™è¨»ï¼ˆå¯ç©ºç™½ï¼‰\")\n",
        "            btn_end_break = gr.Button(\"â¹ï¸ çµæŸä¼‘æ¯ä¸¦è¨˜éŒ„\")\n",
        "        msg_pomo = gr.Markdown()\n",
        "        # ã€ä¿®æ­£ã€‘ç§»é™¤ height åƒæ•¸\n",
        "        grid_logs = gr.Dataframe(value=logs_df, label=\"ç•ªèŒ„é˜ç´€éŒ„\", interactive=False)\n",
        "\n",
        "    with gr.Tab(\"AI Plan\"):\n",
        "        gr.Markdown(\"æŠŠ**ä»Šå¤©çš„ä»»å‹™**æ’æˆ **morning / afternoon / evening** ä¸‰æ®µè¡Œå‹•è¨ˆç•«ã€‚è‹¥æœªè¨­ GEMINI_API_KEYï¼Œæœƒç”¨è¦å‰‡å¼ã€‚\")\n",
        "        btn_plan = gr.Button(\"ğŸ§  ç”¢ç”Ÿä»Šæ—¥è¨ˆç•«\")\n",
        "        out_plan = gr.Markdown()\n",
        "\n",
        "    with gr.Tab(\"Crawler\"):\n",
        "        gr.Markdown(\"# PTT å…«å¦ç‰ˆåˆ†æå™¨\")\n",
        "        gr.Markdown(\"å¾ PTT å…«å¦ç‰ˆçˆ¬å–æœ€æ–°æ–‡ç« ï¼Œä¸¦é€²è¡Œé—œéµè©åˆ†æã€‚\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"### 1. çˆ¬å–æ–‡ç« \")\n",
        "                ptt_pages = gr.Number(value=3, label=\"çˆ¬å–é æ•¸\", precision=0)\n",
        "                ptt_push = gr.Number(value=20, label=\"æœ€å°‘æ¨æ–‡æ•¸\", precision=0)\n",
        "                ptt_keyword = gr.Textbox(label=\"æ¨™é¡Œé—œéµè© (å¯ç©ºç™½)\")\n",
        "                btn_crawl_ptt = gr.Button(\"ğŸ•·ï¸ é–‹å§‹çˆ¬å– PTT\")\n",
        "                msg_ptt_crawl = gr.Markdown()\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"### 2. åˆ†ææ–‡æœ¬\")\n",
        "                gr.Markdown(\"ï¼ˆæœƒè‡ªå‹•åˆ†æä¸‹æ–¹ã€Œçˆ¬èŸ²åŸå§‹æ–‡ç« ã€åˆ—è¡¨ä¸­çš„æ‰€æœ‰å…§å®¹ï¼‰\")\n",
        "                # ã€ä¿®æ”¹ã€‘é è¨­å€¼æ”¹ç‚º 5\n",
        "                ptt_topk = gr.Number(value=5, label=\"Top K é—œéµè©\", precision=0)\n",
        "                ptt_mindf = gr.Number(value=2, label=\"æœ€å°æ–‡ä»¶é »ç‡ (min_df)\", precision=0)\n",
        "                btn_analyze_ptt = gr.Button(\"ğŸ“Š é–‹å§‹åˆ†ææ–‡æœ¬\")\n",
        "                msg_ptt_analyze = gr.Markdown()\n",
        "\n",
        "        gr.Markdown(\"---\")\n",
        "        gr.Markdown(\"### åˆ†ææ‘˜è¦ (Top K èˆ‡ Bigrams)\")\n",
        "        out_ptt_summary = gr.Markdown(\" (é»æ“Šã€Œé–‹å§‹åˆ†ææ–‡æœ¬ã€å¾Œé¡¯ç¤º)\")\n",
        "\n",
        "        gr.Markdown(\"### é—œéµè©çµ±è¨ˆè¡¨ (Terms)\")\n",
        "        # ã€ä¿®æ­£ã€‘ç§»é™¤ height åƒæ•¸\n",
        "        grid_ptt_terms = gr.Dataframe(value=terms_df, label=\"é—œéµè©çµ±è¨ˆ\", interactive=False)\n",
        "\n",
        "        gr.Markdown(\"### çˆ¬èŸ²åŸå§‹æ–‡ç«  (Posts)\")\n",
        "        # ã€ä¿®æ­£ã€‘ç§»é™¤ height åƒæ•¸\n",
        "        grid_ptt_posts = gr.Dataframe(value=ptt_posts_df, label=\"çˆ¬å–æ–‡ç« åˆ—è¡¨\", interactive=False)\n",
        "\n",
        "    with gr.Tab(\"Summary\"):\n",
        "        btn_summary = gr.Button(\"ğŸ“Š é‡æ–°è¨ˆç®—ä»Šæ—¥å®Œæˆç‡\")\n",
        "        out_summary2 = gr.Markdown()\n",
        "\n",
        "    # === ç¶å®šå‹•ä½œ (å·²ä¿®æ­£ç¸®æ’) ===\n",
        "\n",
        "    btn_refresh.click(\n",
        "        _refresh,\n",
        "        outputs=[\n",
        "            grid_tasks, grid_logs, task_choice, out_summary,\n",
        "            grid_ptt_posts, grid_ptt_terms, sel_task\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    btn_add.click(\n",
        "        add_task,\n",
        "        inputs=[task, priority, est_min, due_date, labels, notes, planned_for],\n",
        "        outputs=[msg_add, grid_tasks, task_choice, sel_task]\n",
        "    )\n",
        "\n",
        "    btn_update.click(\n",
        "        update_task_status,\n",
        "        inputs=[task_choice, new_status],\n",
        "        outputs=[msg_update, grid_tasks, task_choice, sel_task]\n",
        "    )\n",
        "\n",
        "    btn_done.click(\n",
        "        mark_done,\n",
        "        inputs=[task_choice],\n",
        "        outputs=[msg_update, grid_tasks, task_choice, sel_task]\n",
        "    )\n",
        "\n",
        "    btn_start_work.click(\n",
        "        start_phase, inputs=[sel_task, gr.State(\"work\"), cycles], outputs=[msg_pomo]\n",
        "    )\n",
        "    btn_end_work.click(\n",
        "        end_phase, inputs=[sel_task, note_work], outputs=[msg_pomo]\n",
        "    )\n",
        "    btn_start_break.click(\n",
        "        start_phase, inputs=[sel_task, gr.State(\"break\"), cycles], outputs=[msg_pomo]\n",
        "    )\n",
        "    btn_end_break.click(\n",
        "        end_phase, inputs=[sel_task, note_break], outputs=[msg_pomo]\n",
        "    )\n",
        "\n",
        "    btn_plan.click(generate_today_plan, outputs=[out_plan])\n",
        "\n",
        "    btn_crawl_ptt.click(\n",
        "        crawl_ptt_board,\n",
        "        inputs=[ptt_pages, ptt_push, ptt_keyword],\n",
        "        outputs=[msg_ptt_crawl, grid_ptt_posts]\n",
        "    )\n",
        "\n",
        "    btn_analyze_ptt.click(\n",
        "        analyze_ptt_texts,\n",
        "        inputs=[ptt_topk, ptt_mindf],\n",
        "        outputs=[msg_ptt_analyze, grid_ptt_terms, out_ptt_summary]\n",
        "    )\n",
        "\n",
        "    btn_summary.click(today_summary, outputs=[out_summary2])"
      ],
      "metadata": {
        "id": "LfHeUi4MrYPY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 934
        },
        "id": "bfmVePsjsGCZ",
        "outputId": "6064cf79-5b28-43c0-f1a0-2e339dfa6922"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://10f33ab9b84858dfca.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://10f33ab9b84858dfca.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "æ­£åœ¨çˆ¬å–ç¬¬ 1/3 é ...\n",
            "æ­£åœ¨çˆ¬å–ç¬¬ 2/3 é ...\n",
            "æ­£åœ¨çˆ¬å–ç¬¬ 3/3 é ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /tmp/jieba.cache\n",
            "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "æ­£åœ¨åˆ†æ 6 ç¯‡æ–‡ç« ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading model cost 1.084 seconds.\n",
            "DEBUG:jieba:Loading model cost 1.084 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "DEBUG:jieba:Prefix dict has been built successfully.\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "æ­£åœ¨å‘¼å« Gemini ç”¢ç”Ÿæ´å¯Ÿæ‘˜è¦...\n",
            "Gemini æ´å¯Ÿç”¢å‡ºæˆåŠŸã€‚\n"
          ]
        }
      ]
    }
  ]
}